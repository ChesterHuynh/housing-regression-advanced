---
title: "housing-regression-advanced-modeling"
author: "Andrew Shao"
date: "July 20, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(080599)
library(tidyverse)
library(reshape2)
library(glmnet)
library(e1071)
library(DAAG)
library(MASS)
library(GGally)
library(mice)
library(VIM)
library(randomForest)
library(gbm)
library(xgboost)
library(SuperLearner)
library(caret)
library(pracma)
```

# Data Preprocessing
```{r, eval = FALSE}
train <- read.csv("train.csv", stringsAsFactors = FALSE)
test <- read.csv("test.csv", stringsAsFactors = FALSE)

# Formatting for SVM
## Manual feature engineering for SVM
## Reference1
full <- bind_rows(train, test) #%>%
  #mutate("MSSubClass" = as.factor(MSSubClass)) %>%
  #mutate("YrSold" = as.factor(YrSold)) %>%
  #mutate("MoSold" = as.factor(MoSold))

#full$Functional[is.na(full$Functional)] <- "Typ"
#full$Electrical[is.na(full$Electrical)] <- "SBrkr"
#full$KitchenQual[is.na(full$KitchenQual)] <- "TA"
#full$PoolQC[is.na(full$PoolQC)] <- "None"

SalePrice <- train$SalePrice
Id <- test$Id
full[, c('Id', 'SalePrice')] <- NULL
full_chr <- full[, sapply(full, is.character)]
full_int <- full[, sapply(full, is.integer)]

### Lot Frontage (not touching for now, using mice package)
#full_int[which(is.na(full_int$LotFrontage) == 1), ]
### Alley
#full_chr$Alley[is.na(full_chr$Alley)] <- "None"
### Basement
#full_chr[is.na(full_chr$BsmtExposure), ]
#full_int$BsmtFinSF1[is.na(full_int$BsmtFinSF1)] <- 0
#full_int$BsmtFinSF2[is.na(full_int$BsmtFinSF2)] <- 0
#full_int$BsmtUnfSF[is.na(full_int$BsmtUnfSF)] <- 0
#full_int$TotalBsmtSF[is.na(full_int$TotalBsmtSF)] <- 0
#full_int$BsmtFullBath[is.na(full_int$BsmtFullBath)] <- 0
#full_int$BsmtHalfBath[is.na(full_int$BsmtHalfBath)] <- 0
### Masonry Veneer
#full_chr$MasVnrType[which(is.na(full_int$MasVnrArea) == 1)] <- "None"
#full_chr$MasVnrType[is.na(full_chr$MasVnrType)] <- "Stone"
#full_int$MasVnrArea[is.na(full_int$MasVnrArea)] <- 0
### Garage
#full_int$GarageYrBlt[is.na(full_int$GarageYrBlt)] <- 0
#full_int$GarageCars[is.na(full_int$GarageCars)] <- 0
#full_int$GarageArea[is.na(full_int$GarageArea)] <- 0
full_chr[is.na(full_chr)] <- "Not Available"
full_fac <- mutate_all(full_chr, as.factor) %>% as.data.frame()
full_combined <- bind_cols(full_fac, full_int)
full_transformed <- full_combined %>%
  mutate("LotArea" = log(LotArea + 1)) %>%
  mutate("GrLivArea" = log(GrLivArea + 1)) %>%
  mutate("LotFrontage" = log(LotFrontage + 1)) %>%
  mutate("TotalBsmtSF" = log(TotalBsmtSF + 1)) %>%
  mutate("GarageCars2" = GarageCars^2)

# Formatting train set
train_format <- train[, which(summarise_all(train, is.character) == 1)] %>% 
  mutate_all(as.factor) %>%
  mutate_all(addNA) %>%
  bind_cols(train[, which(summarise_all(train, is.character) == 0)]) %>%
  mutate(MSSubClass = factor(MSSubClass)) %>%
  mutate(OverallQual = factor(OverallQual)) %>%
  mutate(OverallCond = factor(OverallCond))

# Formatting test set
test_format <- test[, which(summarise_all(test, is.character) == 1)] %>% 
  mutate_all(as.factor) %>%
  mutate_all(addNA) %>%
  bind_cols(test[, which(summarise_all(test, is.character) == 0)]) %>%
  mutate(MSSubClass = factor(MSSubClass)) %>%
  mutate(OverallQual = factor(OverallQual)) %>%
  mutate(OverallCond = factor(OverallCond))

# Splitting into train and test
samp_index <- sample(nrow(train_format), (0.7*nrow(train_format)))
train_samp <- train[samp_index, ]
train_samp <- train_samp[, which(summarise_all(train_samp, is.character) == 1)] %>% 
  mutate_all(as.factor) %>%
  mutate_all(addNA) %>%
  bind_cols(train_samp[, which(summarise_all(train_samp, is.character) == 0)]) %>%
  mutate(MSSubClass = factor(MSSubClass)) %>%
  mutate(OverallQual = factor(OverallQual)) %>%
  mutate(OverallCond = factor(OverallCond))
  
test_samp <- train[-samp_index, ]
test_samp <- test_samp[, which(summarise_all(test_samp, is.character) == 1)] %>% 
  mutate_all(as.factor) %>%
  mutate_all(addNA) %>%
  bind_cols(test_samp[, which(summarise_all(test_samp, is.character) == 0)]) %>%
  mutate(MSSubClass = factor(MSSubClass)) %>%
  mutate(OverallQual = factor(OverallQual)) %>%
  mutate(OverallCond = factor(OverallCond))

# Splitting into quantitative and categorical variables
train_sampn <- train_samp[, which(summarise_all(train_samp, is.double) == 1)]
train_testn <- test_samp[, which(summarise_all(train_samp, is.double) == 1)]

# Looking at NA counts
#aggr(train_sampn, numbers = TRUE, sortVars = TRUE, labels = names(train_sampn), cex.axis = 0.7, gap = 0.3, ylab = c("Histogram of missing data", "Pattern"))
apply(train, 2, is.na) %>%
  apply(2, sum)
apply(test, 2, is.na) %>%
  apply(2, sum)
aggr(train, numbers = TRUE, sortVars = TRUE, cex.axis = 0.5, gap = 0.2)

# Data Cleaning
## Imputation
### Imputation of full data
full_mice <- mice(full_combined, method = "rf")
full_complete <- complete(full_mice)
train_rf <- full_complete[1:length(SalePrice), ]
train_rf_outliers = mutate(train_rf, "SalePrice" = SalePrice)
train_rf_outliers <- train_rf[-c(89, 121, 186, 251, 272, 326, 333, 347, 376, 399, 524, 633, 667, 692, 811, 826, 949, 1004, 1012, 1171, 1188, 1231, 1271, 1276, 1299, 1322, 1325, 1371, 1380, 1387, 1424), ]
SalePrice_outliers <- SalePrice[-c(89, 121, 186, 251, 272, 326, 333, 347, 376, 399, 524, 633, 667, 692, 811, 826, 949, 1004, 1012, 1171, 1188, 1231, 1271, 1276, 1299, 1322, 1325, 1371, 1380, 1387, 1424)]
test_rf <- full_complete[(length(SalePrice) + 1):nrow(full), ]

### Imputation of full transformed data
full_trans <- mice(full_transformed, method = "rf")
full_trans_complete <- complete(full_trans)
train_tr_rf <- full_trans_complete[1:length(SalePrice), ]
test_tr_rf <- full_trans_complete[(length(SalePrice) + 1):nrow(full), ]

### Imputation of training sample
temp_train_sampn <- mice(train_sampn[, c(35, 2, 6, 23)], meth = "rf")
summary(temp_train_sampn)
temp_train_sampn_complete <- mice::complete(temp_train_sampn, 1)
xyplot(temp_train_sampn, SalePrice ~ LotFrontage + MasVnrArea + GarageYrBlt, pch = 18, cex = 1, alpha = 0.01)
densityplot(temp_train_sampn)
train_sampn_complete <- mutate(train_sampn, LotFrontage = temp_train_sampn_complete$LotFrontage) %>%
  mutate(MasVnrArea = temp_train_sampn_complete$MasVnrArea) %>%
  mutate(GarageYrBlt = temp_train_sampn_complete$GarageYrBlt)

train_samp_complete <- right_join(train_samp, train_sampn_complete)
```

# Stepwise Model
```{r, eval = FALSE}
# Model 1
lm_step <- lm(log(SalePrice) ~ ., data = train_rf)
step <- stepAIC(lm_step, direction = "both")
summary(step)
## Manually selecting qualitative variables
lm_step_final <- lm(log(SalePrice) ~ LotArea + YearBuilt + YearRemodAdd +
                      MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
                      `1stFlrSF` + `2ndFlrSF` + LowQualFinSF + BsmtFullBath + 
                      FullBath + KitchenAbvGr + TotRmsAbvGrd + Fireplaces + 
                      GarageCars + WoodDeckSF + EnclosedPorch + `3SsnPorch` + 
                      ScreenPorch + PoolArea + MoSold + OverallQual + OverallCond + 
                      MSSubClass + MSZoning + Neighborhood,
                    data = train_samp_complete)
summary(lm_step_final)
## Manually fixing new levels for prediction
test_samp_temp <- test_rf
test_samp_temp$MSZoning[which(test_samp_temp$MSZoning == "Not Available")] <- "RL"
test_samp_temp$Exterior1st[which(test_samp_temp$Exterior1st == "Not Available")] <- "MetalSd"
test_samp_temp$KitchenQual[which(test_samp_temp$KitchenQual == "Not Available")] <- "TA"
test_samp_temp$Functional[which(test_samp_temp$Functional == "Not Available")] <- "Typ"
test_samp_temp$SaleType[which(test_samp_temp$SaleType == "Not Available")] <- "WD"
## 
step_train_predict <- predict(step, train_rf) %>%
  exp() %>% 
  as.data.frame() %>%
  mutate("SalePrice" = SalePrice)
ggplot(step_train_predict, aes(., SalePrice)) + geom_point()
## Prediction
step_predict <- predict(step, test_samp_temp) %>%
  exp()
step_submit <- data.frame("Id" = Id, "SalePrice" = step_predict)
write_csv(step_submit, "step_reg1.csv")
```

# Ridge Regression

Reference 3
```{r, eval = FALSE}
linear_model <- lm(SalePrice_outliers ~ ., train_rf_outliers)
plot(linear_model)
x <- model.matrix(~ ., train_tr_rf)
ylog <- log(SalePrice)
test_matrix <- model.matrix(~ ., test_tr_rf)
lambda <- 10^seq(10, -2, length = 100)
ridge_model <- glmnet(x, ylog, alpha = 0, lambda = lambda)
summary(ridge_model)
cv_out <- cv.glmnet(x, ylog, alpha = 0)
best_lambda <- cv_out$lambda.min
ridge_train_pred <- predict(ridge_model, s = best_lambda, newx = x) %>%
  exp() %>%
  as.data.frame() %>%
  mutate("SalePrice" = SalePrice)
ggplot(ridge_train_pred, aes(`1`, SalePrice)) + geom_point()
ridge_pred <- predict(ridge_model, s = best_lambda, newx = test_matrix) %>%
  exp()
```

# Lasso Regression

Reference 3
```{r, eval = FALSE}
lasso_model <- glmnet(x, ylog, alpha = 1, lambda = lambda)
lasso_train_pred <- predict(lasso_model, s = best_lambda, newx = x) %>%
  exp()
plot(lasso_train_pred, SalePrice)
lasso_coef <- predict(lasso_model, s = best_lambda, type = "coefficients")
lasso_pred <- predict(lasso_model, s = best_lambda, newx = test_matrix) %>%
  exp()
```

# Support Vector Machine

Reference 1, Reference 2
```{r, eval = FALSE}
# Initial Model
svm_model <- svm(SalePrice ~ ., train_tr_rf)
svm_predict <- predict(svm_model, test_tr_rf)

# Tuning
## Binding SalePrice to train_rf to let tune() work
train_tune <- mutate(train_rf, "SalePrice" = SalePrice)
train_tr_tune <- mutate(train_tr_rf, "SalePrice" = SalePrice)
train_tune_outliers <- train_tune[-c(89, 121, 186, 251, 272, 326, 333, 347, 376, 399, 524, 633, 667, 692, 811, 826, 949, 1004, 1012, 1171, 1188, 1231, 1271, 1276, 1299, 1322, 1325, 1371, 1380, 1387, 1424), ]
tuneResult1 <- tune(svm, SalePrice ~ ., data = train_tune, ranges = list(epsilon = seq(0, 1, 0.1), cost = 1:5))
plot(tuneResult1)

best_model <- tuneResult1$best.model
svm_tuned_train <- predict(best_model, train_rf)

plot(svm_tuned_train, SalePrice)

svm_tuned_pred <- predict(best_model, test_rf)

# Submission
svm_solution <- data.frame("Id" = Id, "SalePrice" = svm_tuned_pred)
write_csv(svm_solution, "svm_solution_tuned8.csv")
```
# Boosting

Reference 5, Reference 6
```{r, eval = FALSE}
# Model
boosted_model <- gbm(log(SalePrice) ~ ., data = train_tr_rf, 
                     distribution = "gaussian", n.trees = 5000, 
                     interaction.depth = 4, shrinkage = 0.01)
boosted_model

boosted_train <- predict(boosted_model, n.trees = 5000, train_tr_rf) %>%
  exp()
plot(boosted_train, SalePrice)

# Tuning
## Hyperparameter grid

hyper_grid <- expand.grid(
  shrinkage = c(0.01, 0.1, 0.3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(0.65, 0.8, 1),
  optimal_trees = 0,
  min_rmse = 0
)

# Grid Search

for(i in 1:nrow(hyper_grid)) {
  set.seed(080599)
  
  gbm_tune <- gbm(
    formula = log(SalePrice) ~ .,
    distribution = "gaussian",
    data = train_tr_rf,
    n.trees = 6000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = 0.75,
    n.cores = NULL,
    verbose = FALSE
  )
  
  hyper_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
  hyper_grid$min_rmse[i] <- sqrt(min(gbm_tune$valid.error))
}

hyper_grid %>%
  arrange(min_rmse) %>%
  head(10)

# Final Model

gbm_final <- gbm(
  formula = log(SalePrice) ~ .,
  distribution = "gaussian",
  data = train_tr_rf,
  n.trees = 5998,
  interaction.depth = 3,
  shrinkage = 0.01,
  n.minobsinnode = 15,
  bag.fraction = 1,
  train.fraction = 0.75,
  n.cores = NULL,
  verbose = FALSE
)

final_train <- predict(gbm_final, n.trees = 5998, train_tr_rf) %>%
  exp()
plot(boosted_train, SalePrice)

RMSE(final_train, SalePrice)

boosted_pred <- predict(gbm_final, n.trees = 5998, test_tr_rf) %>%
  exp()
boosted_solution <- data.frame("Id" = Id, "SalePrice" = boosted_pred)
write_csv(boosted_solution, "boosted2.csv")
```

# Random Forest

Reference 5
```{r, eval = FALSE}
rforest_model <- randomForest(SalePrice ~ ., data = train_tr_rf,
                              mtry = 26, importance = TRUE, ntrees = 5000)
rforest_model

rforest_train <- predict(rforest_model, train_tr_rf)
plot(rforest_train, SalePrice)
rforest_pred <- predict(rforest_model, test_tr_rf)
```

# XGBoost

Reference 6
```{r, eval = FALSE}
x_sparse_train <- sparse.model.matrix(SalePrice ~ ., train_tr_rf)
x_sparse_test <- sparse.model.matrix(~ ., test_tr_rf)

# First Model
xgb_model1 <- xgb.cv(data = x_sparse_train, 
                    label = SalePrice, 
                    nrounds = 1000, 
                    nfold = 5,
                    objective = "reg:linear",
                    verbose = 0,
                    early_stopping_rounds = 10)
## Number of trees that minimize error
xgb_model1$evaluation_log %>%
  summarise(
    ntrees_train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse_train = min(train_rmse_mean),
    ntrees.test = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse_test = min(test_rmse_mean))
## Plotting error vs number of trees
ggplot(xgb_model1$evaluation_log) + 
  geom_line(aes(iter, train_rmse_mean), color = "red") + 
  geom_line(aes(iter, test_rmse_mean), color = "blue")

# Second Model
xgb_model2 <- xgb.cv(data = x_sparse_train,
                     label = log(SalePrice),
                     nrounds = 1000,
                     nfold = 5, 
                     objective = "reg:linear",
                     verbose = 0,
                     early_stopping_rounds = 10)
## Plotting error vs number of trees
ggplot(xgb_model2$evaluation_log) + 
  geom_line(aes(iter, train_rmse_mean), color = "red") + 
  geom_line(aes(iter, test_rmse_mean), color = "blue")

# Final Model
xgb_final <- xgboost(x_sparse_train,
                     log(SalePrice), 
                     nrounds = 1000,
                     objective = "reg:linear",
                     verbose = 0)
xgb_train_pred <- predict(xgb_final, x_sparse_train)
plot(xgb_train_pred, SalePrice)

xgb_pred <- predict(xgb_final, x_sparse_test)
xgb_submission <- data.frame("Id" = Id, "SalePrice" = xgb_pred)
write_csv(xgb_submission, "xgb1.csv")
```

# Ensemble Models

Reference 7
```{r, eval = FALSE}
# Dataframe with all model predictions
stacked_train <- data.frame("ridge" = ridge_train_pred$`1`,
                            "lass0" = lasso_train_pred,
                            "svm" = svm_tuned_train,
                            "gbm" = final_train,
                            "rforest" = rforest_train,
                            "xgb" = xgb_train_pred)

# Weighted Averages
weighted_train_pred <- data.frame("Predict" = ((stacked_train$ridge * .1) +
                                               (stacked_train$X1 * .1) + 
                                               (stacked_train$svm * .3) + 
                                               (stacked_train$gbm * .2) + 
                                               (stacked_train$rforest * .1) + 
                                               (stacked_train$xgb * .2)),
                                  "SalePrice" = SalePrice)
ggplot(weighted_train_pred, aes(Predict, SalePrice)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0))
RMSE(weighted_train_pred$Predict, SalePrice)

weighted_test_pred <- data.frame("Id" = Id,
                                 "SalePrice" = ((ridge_pred * .1) +
                                                (lasso_pred * .1) + 
                                                (svm_tuned_pred * .3) + 
                                                (boosted_pred * .2) + 
                                                (rforest_pred * .1) + 
                                                (xgb_pred * .2)))
colnames(weighted_test_pred) <- c("Id", "SalePrice")
write_csv(weighted_test_pred, "weighted_pred1.csv")

# Stacking with Ridge as the controller
stacked_matrix <- model.matrix(SalePrice ~ ., stacked_train)

lambda <- 10^seq(10, -2, length = 100)
stacked_model <- glmnet(stacked_matrix, SalePrice, alpha = 0, lambda = lambda)
summary(stacked_model)
cv_out <- cv.glmnet(x, ylog, alpha = 0)
best_lambda <- cv_out$lambda.min
stacked_train_pred <- predict(stacked_model, s = best_lambda, newx = stacked_matrix) %>%
 # exp() %>%
  as.data.frame() %>%
  mutate("SalePrice" = SalePrice)
ggplot(stacked_train_pred, aes(`1`, SalePrice)) + 
  geom_point() 

# Super Learner
sl_model <- SuperLearner(SalePrice, x, family = gaussian(),
                               v = 5,
                               SL.library = list("SL.svm",
                                                 "SL.glm",
                                                 "SL.xgboost"))
sl_model
plot(sl_model$SL.predict, SalePrice)
sl_predict <- predict.SuperLearner(sl_model, test_rf)
```

# Blending

Reference 1, Reference 8, Reference 9
```{r}
laurenstc <- read.csv("laurenstc.csv")
pca_sub <- read.csv("pca_sub.csv")
blended_pred <- data.frame("Id" = Id, "SalePrice" = nthroot(laurenstc$SalePrice * pca_sub$SalePrice * svm_tuned_pred, 3))
write_csv(blended_pred, "blended_pred1.csv")
```

# Brute Force Adjustments

Reference 4
```{r, eval = FALSE}
q1 <- quantile(weighted_train_pred$Predict, 0.0042)
q2 <- quantile(weighted_train_pred$Predict, 0.99)

weighted_train_brute <- weighted_train_pred

weighted_train_brute$Predict[which(weighted_train_pred$Predict < q1)] <- (weighted_train_brute$Predict[which(weighted_train_pred$Predict < q1)] * 0.77)

weighted_train_brute$Predict[which(weighted_train_pred$Predict > q2)] <- weighted_train_brute$Predict[which(weighted_train_pred$Predict > q2)] * 1.1

ggplot(weighted_train_brute, aes(Predict, SalePrice)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0)

blended_brute_pred <- blended_pred

blended_brute_pred$SalePrice[which(blended_pred$SalePrice < q1)] <- (blended_brute_pred$SalePrice[which(blended_pred$SalePrice < q1)] * 0.79)

blended_brute_pred$SalePrice[which(blended_pred$SalePrice > q2)] <- blended_brute_pred$SalePrice[which(blended_pred$SalePrice > q2)] * 1.1

write_csv(blended_brute_pred, "blended_brute_pred4.csv")
```


References
#1.<https://www.kaggle.com/couyang/hybrid-svm-benchmark-approach-0-11180-lb-top-2>
#2.<https://rstudio-pubs-static.s3.amazonaws.com/280840_d4fb4f186d454d5dbce3ba2cbe4bbcdb.html#tune-svm-regression-model>
#3.<https://www.r-bloggers.com/ridge-regression-and-the-lasso/>
#4.<https://www.kaggle.com/agehsbarg/top-10-0-10943-stacking-mice-and-brutal-force/data>
#5.<https://daviddalpiaz.github.io/r4sl/ensemble-methods.html>
#6.<http://uc-r.github.io/gbm_regression#gbm>
#7.<https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html>
#8.<https://www.kaggle.com/dhimananubhav/top-2-from-laurenstc-on-house-price-prediction>
#9.<https://www.kaggle.com/massquantity/all-you-need-is-pca-lb-0-11421-top-4>






